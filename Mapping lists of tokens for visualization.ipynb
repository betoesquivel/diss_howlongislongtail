{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from linkers_and_taggers import wikifier_tag, stanford_tag, stanford_tagger\n",
    "from sample_data import n_samples\n",
    "from json_extractor import from_file_get_n_docs\n",
    "from similarity import similar, find_similar_to_a_in_dict_b, a_is_not_in_dict_b, compare_linkers_parsed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs = n_samples(10)\n",
    "stan_docs = from_file_get_n_docs('50_tagged_by_stanford.jsonl', 10)\n",
    "wiki_docs = from_file_get_n_docs('100_tagged_by_wikifier.jsonl', 10)\n",
    "len(raw_docs), len(stan_docs), len(wiki_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe_token_array = lambda toks, name=None: (u'{0} {1} tokens'.format(len(toks), name), u'|| '.join(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikifier tokens observations\n",
    "    1. No punctuation as word tokens. This only means that the surface forms only include words and no punctuation. It makes sense for a surface_form to not be split by punctuation. When I want to index stanford's words, I have to skip punctuation. However, I have to keep punctuation for stanford to tag. So, maybe I can make a dictionary that maps the token index in stanford, to the word index.\n",
    "    2. ' are kept as part of the token for It's, I don't know about possesives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'876 wiki tokens',\n",
       " u\"NYMag.com|| Daily|| Intelligencer|| Vulture|| The|| Cut|| Science|| of|| Us|| Grub|| Street|| Bedford|| Bowery|| FOLLOW|| Facebook|| Twitter|| UserName|| LOG|| IN|| REGISTER|| Fashions|| Runway|| Street|| Style|| Designers|| Fame|| Beauty|| Goods|| Love|| War|| search|| Sections|| Fashions|| Fame|| Beauty|| Goods|| Love|| War|| Plus|| Runway|| Street|| Style|| Designers|| Sites|| NYMag.com|| Daily|| Intelligencer|| Vulture|| Science|| of|| Us|| Grub|| Street|| Bedford|| Bowery|| Like|| UsFollow|| Us|| Popular|| on|| The|| Cut|| Ask|| Polly|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing|| Top|| Shows|| Oscar|| de|| la|| Renta|| See|| it|| Michael|| Kors|| See|| it|| Suno|| See|| it|| Coach|| See|| it|| Narciso|| Rodriguez|| See|| it|| Tory|| Burch|| See|| it|| Carolina|| Herrera|| See|| it|| Rodarte|| See|| it|| Diesel|| Black|| Gold|| See|| it|| Jeremy|| Scott|| See|| it|| Thom|| Browne|| See|| it|| rag|| bone|| See|| it|| Tommy|| Hilfiger|| See|| it|| Prabal|| Gurung|| See|| it|| Diane|| Von|| Furstenberg|| See|| it|| keeping|| us|| honest|| September|| 16|| 2015|| 7|| 12|| p.m|| This|| New|| Dating|| App|| Will|| Ruin|| Your|| Internet|| Game|| By|| Allison|| P|| Davis|| Follow|| allisonpdavis|| No|| photos|| over|| six|| hours|| old|| allowed|| 37|| Shares|| Share|| 29|| Tweet|| 8|| Share|| 0|| Pin|| It|| 0|| Share|| Email|| Print|| Share|| Photo|| Uwe|| Umstaetter|| Westend61|| Corbis|| The|| best|| photo|| of|| me|| was|| taken|| about|| a|| few|| years|| ago|| I|| will|| never|| again|| look|| so|| young|| so|| free|| or|| so|| skinny|| as|| I|| was|| in|| that|| shot|| Do|| I|| still|| use|| that|| fairly|| blatant|| misrepresentation|| of|| my|| current|| self|| on|| my|| OKCupid|| profile|| You|| bet|| I|| do|| Unfortunately|| a|| Swedish|| dating|| site|| called|| 7Heaven|| wants|| to|| ruin|| my|| internet|| game|| by|| only|| hosting|| photos|| of|| members|| that|| were|| taken|| in|| the|| last|| six|| hours|| It|| can|| check|| timestamps|| Don't|| even|| try|| to|| trick|| it|| it's|| smarter|| than|| us|| Users|| can|| upload|| new|| ones|| as|| they|| want|| to|| but|| only|| if|| they|| were|| recently|| snapped|| It|| doesn\\u2019t|| matter|| who|| you|| are|| now|| bald|| chubby|| with|| bad|| bangs|| a|| bad|| haircut|| or|| a|| new|| tattoo|| 7Heaven|| only|| wants|| that|| authentic|| current|| self|| presented|| to|| potential|| suitors|| That|| seems|| like|| so|| much|| work|| Trying|| to|| get|| yourself|| dating-site-selfie-ready|| every|| six|| hours|| At|| first|| I|| thought|| Chill|| out|| How|| much|| does|| a|| person|| actually|| change|| in|| six|| hours|| But|| then|| I|| remembered|| the|| disparity|| between|| how|| I|| look|| at|| work|| and|| how|| I|| look|| when|| I|| come|| home|| at|| night|| take|| off|| my|| pants|| remove|| my|| makeup|| and|| fall|| asleep|| on|| the|| couch|| for|| a|| few|| hours|| before|| I|| start|| Tindering|| Maybe|| six|| hours|| is|| too|| long|| a|| time|| period|| Springwise|| Tags:dating|| appsimproved|| dating|| appskeeping|| us|| honestdatinglove|| and|| warMore|| Share|| on|| Facebook|| Tweet|| this|| Story|| Top|| Stories|| Sophie|| Theallet\\u2019s|| Elevated|| Triumph|| Narciso|| Rodriguez\\u2019s|| Decisiveness|| Vote|| for|| the|| Top|| Street-Style|| Star|| Day|| 6|| A|| Hyperreal|| Peek|| Into|| the|| Fashion|| Week|| Funhouse|| Most|| Viewed|| Stories|| Ask|| Polly|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing|| 2k|| Shares|| Share|| Tweet|| Khaleesi|| Is|| the|| Queen|| of|| Going-Out|| Tops|| 3.3k|| Shares|| Share|| Tweet|| Kanye|| Relegates|| Kendall|| to|| Second|| Row|| at|| Yeezy|| Show|| Updated|| 14|| Shares|| Share|| Tweet|| Beware|| the|| Manic|| Pixie|| Dream|| Boyfriend|| 4.1k|| Shares|| Share|| Tweet|| The|| Bisexual|| Grad|| Student|| Laughing|| at|| Fifty|| Shades|| of|| Grey|| 272|| Shares|| Share|| Tweet|| 5|| Things|| You|| Should|| Buy|| at|| Sephora|| This|| Month|| 140|| Shares|| Share|| Tweet|| The|| Best|| Street|| Style|| From|| New|| York|| Fashion|| Week|| 1.2k|| Shares|| Share|| Tweet|| Frolicking|| on|| the|| Edges|| of|| Fashion\\u2019s|| Absurd|| Pool|| 191|| Shares|| Share|| Tweet|| Fashion|| Taps|| Into|| the|| Spiritual|| Side|| of|| Things|| 170|| Shares|| Share|| Tweet|| Airline|| Grounds|| 130|| Crew|| Members|| Over|| Their|| BMIs|| 181|| Shares|| Share|| Tweet|| From|| Our|| Partners|| POPSUGAR|| Fashion|| The|| 8|| Style|| Habits|| of|| Insanely|| Gorgeous|| Supermodels|| HuffPost|| Parents|| Sean|| Penn|| Is|| Adopting|| Charlize|| Theron's|| Son|| HuffPost|| Women|| 15|| Signs|| You're|| With|| A|| Good|| Man|| POPSUGAR|| Beauty|| The|| Eye|| Makeup|| Tip|| That|| Will|| Put|| Plastic|| Surgeons|| Out|| of|| Business|| powered|| by|| PubExchange|| The|| Cut\\u2019s|| Latest|| Love|| and|| War|| Features|| 7|| 12|| p.m.This|| New|| Dating|| App|| Will|| Ruin|| Your|| Internet|| Game|| No|| photos|| over|| six|| hours|| old|| allowed|| 3|| 50|| p.m.Please|| Stop|| With|| the|| Muppet|| Sex|| Is|| Animal|| an|| animal|| in|| the|| sack|| I|| don\\u2019t|| want|| to|| know|| 2|| 59|| p.m.New|| York|| Magazine\\u2019s|| Sex|| Lives|| Podcast|| A|| Cultural|| History|| of|| the|| Bionic|| Dong|| Listen|| to|| this|| week's|| Sex|| Lives|| podcast|| 2|| 25|| p.m.Kate|| McKinnon|| Is|| Repping|| for|| Crazy|| Cat|| Ladies|| There's|| nothing|| to|| be|| ashamed|| of|| 1|| 40|| p.m.Men|| Want|| Their|| Parental|| Leave|| Too|| Damn|| It|| More|| and|| more|| of|| them|| are|| suing|| to|| get|| it|| and|| winning|| 1|| 30|| p.m.Kanye|| Relegates|| Kendall|| to|| Second|| Row|| at|| Yeezy|| Show|| Updated|| What|| blasphemy|| is|| this|| 10|| 02|| a.m.Does|| This|| Standing|| Desk|| Make|| My|| Butt|| Look|| Big|| And|| other|| things|| to|| worry|| about|| now|| that|| sitting|| is|| the|| new|| smoking|| 7|| 59|| a.m.Ask|| Polly|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing|| Writing|| can't|| be|| a|| popularity|| contest|| and|| popularity|| doesn't|| add|| up|| to|| much|| anyway|| Yesterday|| at|| 4|| 50|| p.m.Airline|| Grounds|| 130|| Crew|| Members|| Over|| Their|| BMIs|| But|| the|| BMI|| policy|| doesn't|| actually|| make|| sense|| Yesterday|| at|| 3|| 45|| p.m.Kate|| Spade|| Wins|| Gloria|| Steinem|| in|| Female-Icon|| Arms|| Race|| Joan|| Didion|| was|| booked|| Yesterday|| at|| 1|| 20|| p.m.Is|| There|| a|| Phallus|| in|| This|| Taylor|| Swift|| Corn|| Maze|| Do|| you|| see|| what|| I|| see|| Yesterday|| at|| 10|| 33|| a.m.A|| Salute|| to|| the|| Women|| of|| American|| Ninja|| Warrior|| And|| the|| show|| that's|| become|| an|| unlikely|| athletic|| equalizer|| Yesterday|| at|| 9|| 36|| a.m.Jo\")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tokens = wiki_docs[test_doc_index]['words']\n",
    "describe_token_array(wiki_tokens, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Stanford tokenization scheme I use\n",
    "    1. In doc index no. 2 it keeps the >> (\\xbb) weird chars (with proper encoding of course)\n",
    "    2. In doc index no. 2 it keeps the ... (\\u2026) weird chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'884 stanford tokens',\n",
       " u'NYMag.com|| Daily|| Intelligencer|| Vulture|| The|| Cut|| Science|| of|| Us|| Grub|| Street|| Bedford|| &|| Bowery|| FOLLOW:|| Facebook|| Twitter|| UserName|| LOG|| IN|| REGISTER|| Fashions|| Runway|| Street|| Style|| Designers|| Fame|| Beauty|| Goods|| Love|| &|| War|| search|| Sections|| Fashions|| Fame|| Beauty|| Goods|| Love|| &|| War|| Plus|| Runway|| Street|| Style|| Designers|| Sites|| NYMag.com|| Daily|| Intelligencer|| Vulture|| Science|| of|| Us|| Grub|| Street|| Bedford|| &|| Bowery|| Like|| UsFollow|| Us|| Popular|| on|| The|| Cut|| Ask|| Polly:|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing?|| \\xbb|| Top|| Shows|| Oscar|| de|| la|| Renta|| See|| it|| \\xbb|| Michael|| Kors|| See|| it|| \\xbb|| Suno|| See|| it|| \\xbb|| Coach|| See|| it|| \\xbb|| Narciso|| Rodriguez|| See|| it|| \\xbb|| Tory|| Burch|| See|| it|| \\xbb|| Carolina|| Herrera|| See|| it|| \\xbb|| Rodarte|| See|| it|| \\xbb|| Diesel|| Black|| Gold|| See|| it|| \\xbb|| Jeremy|| Scott|| See|| it|| \\xbb|| Thom|| Browne|| See|| it|| \\xbb|| rag|| &|| bone|| See|| it|| \\xbb|| Tommy|| Hilfiger|| See|| it|| \\xbb|| Prabal|| Gurung|| See|| it|| \\xbb|| Diane|| Von|| Furstenberg|| See|| it|| \\xbb|| keeping|| us|| honest|| September|| 16,|| 2015|| 7:12|| p.m.|| This|| New|| Dating|| App|| Will|| Ruin|| Your|| Internet|| Game|| By|| Allison|| P.|| Davis|| Follow|| @allisonpdavis|| No|| photos|| over|| six|| hours|| old|| allowed.|| 37|| Shares|| Share|| 29|| Tweet|| 8|| Share|| 0|| Pin|| It|| 0|| Share|| Email|| Print|| Share|| Photo:|| Uwe|| UmstaetterWestend61Corbis|| The|| best|| photo|| of|| me|| was|| taken|| about|| a|| few|| years|| ago:|| I|| will|| never|| again|| look|| so|| young,|| so|| free,|| or|| so|| skinny|| as|| I|| was|| in|| that|| shot.|| Do|| I|| still|| use|| that|| fairly|| blatant|| misrepresentation|| of|| my|| current|| self|| on|| my|| OKCupid|| profile?|| You|| bet|| I|| do.|| Unfortunately,|| a|| Swedish|| dating|| site|| called|| 7Heaven|| wants|| to|| ruin|| my|| internet|| game|| by|| only|| hosting|| photos|| of|| members|| that|| were|| taken|| in|| the|| last|| six|| hours.|| It|| can|| check|| timestamps.|| Don\\'t|| even|| try|| to|| trick|| it;|| it\\'s|| smarter|| than|| us.|| Users|| can|| upload|| new|| ones|| as|| they|| want|| to,|| but|| only|| if|| they|| were|| recently|| snapped.|| It|| doesn\\u2019t|| matter|| who|| you|| are|| now|| \\u2014|| bald,|| chubby,|| with|| bad|| bangs,|| a|| bad|| haircut,|| or|| a|| new|| tattoo|| \\u2014|| 7Heaven|| only|| wants|| that|| authentic,|| current|| self|| presented|| to|| potential|| suitors.|| That|| seems|| like|| so|| much|| work!|| Trying|| to|| get|| yourself|| dating-site-selfie-ready|| every|| six|| hours?|| At|| first|| I|| thought,|| Chill|| out!|| How|| much|| does|| a|| person|| actually|| change|| in|| six|| hours?|| But|| then|| I|| remembered|| the|| disparity|| between|| how|| I|| look|| at|| work|| and|| how|| I|| look|| when|| I|| come|| home|| at|| night,|| take|| off|| my|| pants,|| remove|| my|| makeup,|| and|| fall|| asleep|| on|| the|| couch|| for|| a|| few|| hours|| before|| I|| start|| Tindering.|| Maybe|| six|| hours|| is|| too|| long|| a|| time|| period.|| Springwise|| Tags:dating|| appsimproved|| dating|| appskeeping|| us|| honestdatinglove|| and|| warMore|| Share|| on|| Facebook|| Tweet|| this|| Story|| Top|| Stories|| Sophie|| Theallet\\u2019s|| Elevated|| Triumph;|| Narciso|| Rodriguez\\u2019s|| Decisiveness|| Vote|| for|| the|| Top|| Street-Style|| Star,|| Day|| 6|| A|| Hyperreal|| Peek|| Into|| the|| Fashion|| Week|| Funhouse|| Most|| Viewed|| Stories|| Ask|| Polly:|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing?|| 2k|| Shares|| Share|| Tweet|| Khaleesi|| Is|| the|| Queen|| of|| Going-Out|| Tops|| 3.3k|| Shares|| Share|| Tweet|| Kanye|| Relegates|| Kendall|| to|| Second|| Row|| at|| Yeezy|| Show|| [Updated]|| 14|| Shares|| Share|| Tweet|| Beware|| the|| Manic|| Pixie|| Dream|| Boyfriend|| 4.1k|| Shares|| Share|| Tweet|| The|| Bisexual|| Grad|| Student|| Laughing|| at|| Fifty|| Shades|| of|| Grey|| 272|| Shares|| Share|| Tweet|| 5|| Things|| You|| Should|| Buy|| at|| Sephora|| This|| Month|| 140|| Shares|| Share|| Tweet|| The|| Best|| Street|| Style|| From|| New|| York|| Fashion|| Week|| 1.2k|| Shares|| Share|| Tweet|| Frolicking|| on|| the|| Edges|| of|| Fashion\\u2019s|| Absurd|| Pool|| 191|| Shares|| Share|| Tweet|| Fashion|| Taps|| Into|| the|| Spiritual|| Side|| of|| Things|| 170|| Shares|| Share|| Tweet|| Airline|| Grounds|| 130|| Crew|| Members|| Over|| Their|| BMIs|| 181|| Shares|| Share|| Tweet|| From|| Our|| Partners|| POPSUGAR|| Fashion|| The|| 8|| Style|| Habits|| of|| Insanely|| Gorgeous|| Supermodels|| HuffPost|| Parents|| Sean|| Penn|| Is|| Adopting|| Charlize|| Theron\\'s|| Son|| HuffPost|| Women|| 15|| Signs|| You\\'re|| With|| A|| Good|| Man|| POPSUGAR|| Beauty|| The|| Eye|| Makeup|| Tip|| That|| Will|| Put|| Plastic|| Surgeons|| Out|| of|| Business|| powered|| by|| PubExchange|| The|| Cut\\u2019s|| Latest|| Love|| and|| War|| Features|| 7:12|| p.m.This|| New|| Dating|| App|| Will|| Ruin|| Your|| Internet|| Game|| No|| photos|| over|| six|| hours|| old|| allowed.|| 3:50|| p.m.Please,|| Stop|| With|| the|| Muppet|| Sex.|| Is|| Animal|| an|| animal|| in|| the|| sack?|| I|| don\\u2019t|| want|| to|| know.|| 2:59|| p.m.New|| York|| Magazine\\u2019s|| \\u2018Sex|| Lives\\u2019|| Podcast:|| A|| Cultural|| History|| of|| the|| Bionic|| Dong|| Listen|| to|| this|| week\\'s|| \"Sex|| Lives\"|| podcast.|| 2:25|| p.m.Kate|| McKinnon|| Is|| Repping|| for|| Crazy|| Cat|| Ladies|| There\\'s|| nothing|| to|| be|| ashamed|| of.|| 1:40|| p.m.Men|| Want|| Their|| Parental|| Leave,|| Too,|| Damn|| It|| More|| and|| more|| of|| them|| are|| suing|| to|| get|| it|| \\u2014|| and|| winning.|| 1:30|| p.m.Kanye|| Relegates|| Kendall|| to|| Second|| Row|| at|| Yeezy|| Show|| [Updated]|| What|| blasphemy|| is|| this?|| 10:02|| a.m.Does|| This|| Standing|| Desk|| Make|| My|| Butt|| Look|| Big?|| And|| other|| things|| to|| worry|| about|| now|| that|| sitting|| is|| the|| new|| smoking.|| 7:59|| a.m.Ask|| Polly:|| Should|| I|| Just|| Give|| Up|| on|| My|| Writing?|| Writing|| can\\'t|| be|| a|| popularity|| contest,|| and|| popularity|| doesn\\'t|| add|| up|| to|| much|| anyway.|| Yesterday|| at|| 4:50|| p.m.Airline|| Grounds|| 130|| Crew|| Members|| Over|| Their|| BMIs|| But|| the|| BMI|| policy|| doesn\\'t|| actually|| make|| sense.|| Yesterday|| at|| 3:45|| p.m.Kate|| Spade|| Wins|| Gloria|| Steinem|| in|| Female-Icon|| Arms|| Race|| Joan|| Didion|| was|| booked.|| Yesterday|| at|| 1:20|| p.m.Is|| There|| a|| Phallus|| in|| This|| Taylor|| Swift|| Corn|| Maze?|| Do|| you|| see|| what|| I|| see?|| Yesterday|| at|| 10:33|| a.m.A|| Salute|| to|| the|| Women|| of|| American|| Ninja|| Warrior|| And|| the|| show|| that\\'s|| become|| an|| unlikely|| athletic|| equalizer.|| Yesterday|| at|| 9:36|| a.m.Jo')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_tokens = [t[0] for t in stan_docs[test_doc_index]]\n",
    "describe_token_array(stan_tokens, 'stanford')\n",
    "# print describe_token_array(stan_tokens, 'stanford')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have two options for dealing with differences due to special characters in text.\n",
    "    1. Pre process the text for stanford to remove all special characters.\n",
    "    2. Keep the special characters and build a function that looks for similar words in a token range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to map array a to array b of different length both arrays sharing a subset of values in the set a + b\n",
    "\n",
    "This way, I can find what window of tokens did the linker or taggers found.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aaaa', u\"aaaa's\", 0, 0), (u'a.', u'a', 1, 2), (u'hola', u'hola!!', 2, 3), (u'perro', u'perro', 3, 6)]\n"
     ]
    }
   ],
   "source": [
    "a = [u'aaaa', u'a.', u'hola', u'perro']\n",
    "b = [u\"aaaa's\", u'!!!', u'a', u'hola!!', u'...', u',', u'perro']\n",
    "\n",
    "expected = [0, 2, 3, 4]\n",
    "\n",
    "calls = 0\n",
    "def map_token_list_a_to_b(a, b, a_shifts = 0, b_shifts = 0):\n",
    "    global calls\n",
    "    calls += 1\n",
    "    difference = len(a) - len(b)\n",
    "    a_index = 0\n",
    "    b_index = 0\n",
    "    \n",
    "    indexes = list()\n",
    "    if not a:\n",
    "        return []\n",
    "    \n",
    "    if not b:\n",
    "        indexes.extend([None for i in range(abs(difference))])\n",
    "        return indexes\n",
    "    \n",
    "    if similar(a[0], b[0], 0.6):\n",
    "        pair = (a[0], b[0], a_shifts, b_shifts)\n",
    "        indexes.append(pair)\n",
    "        indexes.extend(map_token_list_a_to_b(a[1:], b[1:], a_shifts + 1, b_shifts + 1))\n",
    "        return indexes\n",
    "    else:\n",
    "        if abs(difference) > 8:\n",
    "            return indexes\n",
    "        \n",
    "        shift_a = map_token_list_a_to_b(a[1:], b, a_shifts + 1, b_shifts) if len(a) > 1 and difference >= 0 else None\n",
    "        shift_b = map_token_list_a_to_b(a, b[1:], a_shifts, b_shifts + 1) if len(b) > 1 and difference < 0 else None\n",
    "        \n",
    "        if shift_a and shift_b:\n",
    "            print \"Both returned\\na={0}\\nb={1}\".format(shift_a, shift_b)\n",
    "            indexes.extend(shift_a if len(shift_a) >= len(shift_b) else shift_b)\n",
    "            return indexes\n",
    "        elif shift_a:\n",
    "            indexes.extend(shift_a)\n",
    "            return indexes\n",
    "        elif shift_b:\n",
    "            indexes.extend(shift_b)\n",
    "            return indexes\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "print map_token_list_a_to_b(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aaaa', 0, u\"aaaa's\", 0), (u'a.', 1, u'a', 2), (u'hola', 2, u'hola!!', 3), (u'perro', 5, u'perro', 4)]\n"
     ]
    }
   ],
   "source": [
    "valid_index = lambda i, arr: i < len(arr)\n",
    "\n",
    "def should_shift_list (arr, i, shifts, max_shifts, previous_similar):\n",
    "    return valid_index(i+shifts, arr) and shifts <= max_shifts and not previous_similar\n",
    "\n",
    "def compare_a_to_b_shifting_max_from_indexes (a, b, max_shifts, a_i, b_i):\n",
    "    \n",
    "    next_shift = 0\n",
    "    current_shift = 0\n",
    "    are_similar = False\n",
    "    while should_shift_list(a, a_i, next_shift, max_shifts, are_similar):\n",
    "        are_similar = similar(a[a_i + next_shift], b[b_i], 0.6)\n",
    "        current_shift = next_shift\n",
    "        next_shift += 1    \n",
    "    \n",
    "    current_shift = -1 if not are_similar else current_shift\n",
    "    return current_shift\n",
    "\n",
    "def shift_indexes(a_i, b_i, a_shifts, b_shifts):\n",
    "    if a_shifts > 0 and b_shifts > 0:\n",
    "        if a_shifts <= b_shifts:\n",
    "            a_i += a_shifts\n",
    "        else:\n",
    "            b_i += b_shifts\n",
    "    elif a_shifts > 0:\n",
    "        a_i += a_shifts\n",
    "    else:\n",
    "        b_i += b_shifts\n",
    "    \n",
    "    return a_i, b_i\n",
    "\n",
    "def update_max_shifts(max_shifts, a_shifts, b_shifts):\n",
    "    return max_shifts - max([a_shifts, b_shifts])\n",
    "\n",
    "def map_token_list_a_to_b(a, b):\n",
    "    max_shifts = abs(len(a) - len(b)) + 1\n",
    "    a_index = 0\n",
    "    b_index = 0\n",
    "    \n",
    "    mapping = list()\n",
    "    still_similar = True\n",
    "    while (a_index < len(a) or b_index < len(b) and still_similar):\n",
    "        \n",
    "        a_index = a_index if a_index < len(a) else len(a) - 1\n",
    "        b_index = b_index if b_index < len(b) else len(b) - 1\n",
    "        \n",
    "        a_shifts = compare_a_to_b_shifting_max_from_indexes(a, b, max_shifts, a_index, b_index)\n",
    "        b_shifts = compare_a_to_b_shifting_max_from_indexes(b, a, max_shifts, b_index, a_index)\n",
    "        \n",
    "        if a_shifts == -1 and b_shifts == -1:\n",
    "            still_similar = False\n",
    "        else:\n",
    "            a_index, b_index = shift_indexes(a_index, b_index, a_shifts, b_shifts)\n",
    "            max_shifts = update_max_shifts(max_shifts, a_shifts, b_shifts)\n",
    "            a_to_b = (a[a_index], a_index, b[b_index], b_index)\n",
    "            mapping.append(a_to_b)\n",
    "        \n",
    "        a_index += 1\n",
    "        b_index += 1\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "a = [u'aaaa', u'a.', u'hola', u'..', u',', u'perro']\n",
    "b = [u\"aaaa's\", u'!!!', u'a', u'hola!!', u'perro']\n",
    "expected = [0, 2, 3, 4]\n",
    " \n",
    "print map_token_list_a_to_b(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 876)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stan_tokens), len(wiki_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stan_to_wiki = map_token_list_a_to_b(stan_tokens, wiki_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print u\"\\n\".join([u\"Stan={0}; Wiki={1}\".format(stan_tokens[m[1]], wiki_tokens[m[3]]) for m in stan_to_wiki if not similar(stan_tokens[m[1]], wiki_tokens[m[3]], 0.6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'NYMag.com', 0, u'NYMag.com', 0),\n",
       " (u'Daily', 1, u'Daily', 1),\n",
       " (u'Intelligencer', 2, u'Intelligencer', 2),\n",
       " (u'Vulture', 3, u'Vulture', 3),\n",
       " (u'The', 4, u'The', 4),\n",
       " (u'Cut', 5, u'Cut', 5),\n",
       " (u'Science', 6, u'Science', 6),\n",
       " (u'of', 7, u'of', 7),\n",
       " (u'Us', 8, u'Us', 8),\n",
       " (u'Grub', 9, u'Grub', 9),\n",
       " (u'Street', 10, u'Street', 10),\n",
       " (u'Bedford', 11, u'Bedford', 11),\n",
       " (u'Bowery', 13, u'Bowery', 12),\n",
       " (u'FOLLOW:', 14, u'FOLLOW', 13),\n",
       " (u'Facebook', 15, u'Facebook', 14),\n",
       " (u'Twitter', 16, u'Twitter', 15),\n",
       " (u'UserName', 17, u'UserName', 16),\n",
       " (u'LOG', 18, u'LOG', 17),\n",
       " (u'IN', 19, u'IN', 18),\n",
       " (u'REGISTER', 20, u'REGISTER', 19),\n",
       " (u'Fashions', 21, u'Fashions', 20),\n",
       " (u'Runway', 22, u'Runway', 21),\n",
       " (u'Street', 23, u'Street', 22),\n",
       " (u'Style', 24, u'Style', 23),\n",
       " (u'Designers', 25, u'Designers', 24),\n",
       " (u'Fame', 26, u'Fame', 25),\n",
       " (u'Beauty', 27, u'Beauty', 26),\n",
       " (u'Goods', 28, u'Goods', 27),\n",
       " (u'Love', 29, u'Love', 28),\n",
       " (u'War', 31, u'War', 29),\n",
       " (u'search', 32, u'search', 30),\n",
       " (u'Sections', 33, u'Sections', 31),\n",
       " (u'Fashions', 34, u'Fashions', 32),\n",
       " (u'Fame', 35, u'Fame', 33),\n",
       " (u'Beauty', 36, u'Beauty', 34),\n",
       " (u'Goods', 37, u'Goods', 35),\n",
       " (u'Love', 38, u'Love', 36),\n",
       " (u'War', 40, u'War', 37),\n",
       " (u'Plus', 41, u'Plus', 38),\n",
       " (u'Runway', 42, u'Runway', 39),\n",
       " (u'Street', 43, u'Street', 40),\n",
       " (u'Style', 44, u'Style', 41),\n",
       " (u'Designers', 45, u'Designers', 42),\n",
       " (u'Sites', 46, u'Sites', 43),\n",
       " (u'NYMag.com', 47, u'NYMag.com', 44),\n",
       " (u'Daily', 48, u'Daily', 45),\n",
       " (u'Intelligencer', 49, u'Intelligencer', 46),\n",
       " (u'Vulture', 50, u'Vulture', 47),\n",
       " (u'Science', 51, u'Science', 48),\n",
       " (u'of', 52, u'of', 49),\n",
       " (u'Us', 53, u'Us', 50),\n",
       " (u'Grub', 54, u'Grub', 51),\n",
       " (u'Street', 55, u'Street', 52),\n",
       " (u'Bedford', 56, u'Bedford', 53),\n",
       " (u'Bowery', 58, u'Bowery', 54),\n",
       " (u'Like', 59, u'Like', 55),\n",
       " (u'UsFollow', 60, u'UsFollow', 56),\n",
       " (u'Us', 61, u'Us', 57),\n",
       " (u'Popular', 62, u'Popular', 58),\n",
       " (u'on', 63, u'on', 59),\n",
       " (u'The', 64, u'The', 60),\n",
       " (u'Cut', 65, u'Cut', 61),\n",
       " (u'Ask', 66, u'Ask', 62),\n",
       " (u'Polly:', 67, u'Polly', 63),\n",
       " (u'Should', 68, u'Should', 64),\n",
       " (u'I', 69, u'I', 65),\n",
       " (u'Just', 70, u'Just', 66),\n",
       " (u'Give', 71, u'Give', 67),\n",
       " (u'Up', 72, u'Up', 68),\n",
       " (u'on', 73, u'on', 69),\n",
       " (u'My', 74, u'My', 70),\n",
       " (u'Writing?', 75, u'Writing', 71),\n",
       " (u'Top', 77, u'Top', 72),\n",
       " (u'Shows', 78, u'Shows', 73),\n",
       " (u'Oscar', 79, u'Oscar', 74),\n",
       " (u'de', 80, u'de', 75),\n",
       " (u'la', 81, u'la', 76),\n",
       " (u'Renta', 82, u'Renta', 77),\n",
       " (u'See', 83, u'See', 78),\n",
       " (u'it', 84, u'it', 79),\n",
       " (u'Michael', 86, u'Michael', 80),\n",
       " (u'Kors', 87, u'Kors', 81),\n",
       " (u'See', 88, u'See', 82),\n",
       " (u'it', 89, u'it', 83),\n",
       " (u'Suno', 91, u'Suno', 84),\n",
       " (u'See', 92, u'See', 85),\n",
       " (u'it', 93, u'it', 86),\n",
       " (u'Coach', 95, u'Coach', 87),\n",
       " (u'See', 96, u'See', 88),\n",
       " (u'it', 97, u'it', 89),\n",
       " (u'Narciso', 99, u'Narciso', 90),\n",
       " (u'Rodriguez', 100, u'Rodriguez', 91),\n",
       " (u'See', 101, u'See', 92),\n",
       " (u'it', 102, u'it', 93),\n",
       " (u'See', 121, u'See', 112),\n",
       " (u'it', 122, u'it', 113),\n",
       " (u'See', 137, u'See', 128),\n",
       " (u'it', 138, u'it', 129),\n",
       " (u'See', 142, u'See', 133),\n",
       " (u'it', 143, u'it', 134),\n",
       " (u'It', 281, u'it', 272),\n",
       " (u'It', 311, u'with', 302),\n",
       " (u'I', 359, u'in', 350),\n",
       " (u'in', 370, u'I', 361),\n",
       " (u'Shares', 479, u'Shares', 470),\n",
       " (u'Share', 480, u'Share', 471),\n",
       " (u'Tweet', 481, u'Tweet', 472),\n",
       " (u'Shares', 504, u'Share', 495),\n",
       " (u'Fashion', 551, u'Fashion\\u2019s', 542),\n",
       " (u'Share', 555, u'Shares', 546),\n",
       " (u'Share', 567, u'Shares', 558),\n",
       " (u'Share', 579, u'Shares', 570),\n",
       " (u'Their', 587, u'The', 578),\n",
       " (u'now', 773, u'new', 764),\n",
       " (u'Their', 814, u'the', 805),\n",
       " (u'BMIs', 815, u'BMI', 806),\n",
       " (u'a.m.A', 862, u'a.m.A', 853),\n",
       " (u'Salute', 863, u'Salute', 854),\n",
       " (u'to', 864, u'to', 855),\n",
       " (u'the', 865, u'the', 856),\n",
       " (u'Women', 866, u'Women', 857),\n",
       " (u'of', 867, u'of', 858),\n",
       " (u'American', 868, u'American', 859),\n",
       " (u'Ninja', 869, u'Ninja', 860),\n",
       " (u'Warrior', 870, u'Warrior', 861),\n",
       " (u'And', 871, u'And', 862),\n",
       " (u'the', 872, u'the', 863),\n",
       " (u'show', 873, u'show', 864),\n",
       " (u\"that's\", 874, u\"that's\", 865),\n",
       " (u'become', 875, u'become', 866),\n",
       " (u'an', 876, u'an', 867),\n",
       " (u'unlikely', 877, u'unlikely', 868),\n",
       " (u'athletic', 878, u'athletic', 869),\n",
       " (u'equalizer.', 879, u'equalizer', 870),\n",
       " (u'Yesterday', 880, u'Yesterday', 871),\n",
       " (u'at', 881, u'at', 872)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_to_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
