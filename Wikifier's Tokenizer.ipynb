{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from linkers_and_taggers import wikifier_tag, stanford_tag, stanford_tagger\n",
    "from sample_data import n_samples\n",
    "from json_extractor import from_file_get_n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs = n_samples(10)\n",
    "stan_docs = from_file_get_n_docs('50_tagged_by_stanford.jsonl', 10)\n",
    "wiki_docs = from_file_get_n_docs('100_tagged_by_wikifier.jsonl', 10)\n",
    "len(raw_docs), len(stan_docs), len(wiki_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe_token_array = lambda toks, name=None: (u'{0} {1} tokens'.format(len(toks), name), u'|| '.join(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikifier tokens observations\n",
    "    1. No punctuation as word tokens. This only means that the surface forms only include words and no punctuation. It makes sense for a surface_form to not be split by punctuation. When I want to index stanford's words, I have to skip punctuation. However, I have to keep punctuation for stanford to tag. So, maybe I can make a dictionary that maps the token index in stanford, to the word index.\n",
    "    2. ' are kept as part of the token for It's, I don't know about possesives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'118 wiki tokens',\n",
       " u'Home|| Style|| The|| Return|| Of|| The|| Nike|| Air|| Max|| Sensation|| Has|| 80|| s|| Babies|| Hyped|| Posted|| on|| Sep|| 22|| 2015|| If|| you|| were|| a|| basketball|| fan|| who|| was|| born|| in|| the|| 80s|| you|| were|| lucky|| enough|| to|| witness|| the|| beauty|| that|| is|| 90s|| basketball|| It|| was|| truly|| a|| great|| time|| to|| be|| basketball|| fan|| If|| you|| played|| close|| attention|| to|| what|| the|| players|| were|| wearing|| on|| their|| feet|| you|| would|| have|| also|| noticed|| the|| wide|| array|| of|| footwear|| these|| player|| used|| to|| rock|| One|| of|| those|| happens|| to|| the|| the|| Nike|| Air|| Max|| Sensation|| which|| is|| also|| set|| to|| receive|| the|| retro|| treatment|| this|| year|| Originally|| released|| back|| in|| read|| more|| Author|| KicksOnFire|| Share|| This|| Post|| On|| GoogleFacebookTwitter')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tokens = wiki_docs[test_doc_index]['words']\n",
    "describe_token_array(wiki_tokens, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Stanford tokenization scheme I use\n",
    "    1. In doc index no. 2 it keeps the >> (\\xbb) weird chars (with proper encoding of course)\n",
    "    2. In doc index no. 2 it keeps the ... (\\u2026) weird chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'119 stanford tokens',\n",
       " u'Home|| \\xbb|| Style|| \\xbb|| The|| Return|| Of|| The|| Nike|| Air|| Max|| Sensation|| Has|| 80\\u2019s|| Babies|| Hyped!|| Posted|| on|| Sep|| 22,|| 2015|| If|| you|| were|| a|| basketball|| fan|| who|| was|| born|| in|| the|| 80s,|| you|| were|| lucky|| enough|| to|| witness|| the|| beauty|| that|| is|| 90s|| basketball.|| It|| was|| truly|| a|| great|| time|| to|| be|| basketball|| fan.|| If|| you|| played|| close|| attention|| to|| what|| the|| players|| were|| wearing|| on|| their|| feet|| you|| would|| have|| also|| noticed|| the|| wide|| array|| of|| footwear|| these|| player|| used|| to|| rock.|| One|| of|| those|| happens|| to|| the|| the|| Nike|| Air|| Max|| Sensation,|| which|| is|| also|| set|| to|| receive|| the|| retro|| treatment|| this|| year!|| Originally|| released|| back|| in|| \\u2026read|| more|| Author:|| KicksOnFire|| Share|| This|| Post|| On|| GoogleFacebookTwitter')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_tokens = [t[0] for t in stan_docs[test_doc_index]]\n",
    "describe_token_array(stan_tokens, 'stanford')\n",
    "# print describe_token_array(stan_tokens, 'stanford')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have two options for dealing with differences due to special characters in text.\n",
    "    1. Pre process the text for stanford to remove all special characters.\n",
    "    2. Keep the special characters and build a function that looks for similar words in a token range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option no. 2:  Keep the special chars, build a func that looks for similar words in range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
